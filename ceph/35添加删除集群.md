# 添加服务器
```
1.先添加仓库源 

2.初始化 node 节点过程
# ceph-deploy install --release pacific ceph-node4 

3.擦除磁盘
# ceph-deploy disk zap ceph-node4 /dev/sdb
# ceph-deploy disk zap ceph-node4 /dev/sdc
# ceph-deploy disk zap ceph-node4 /dev/sdd

4.添加 osd
# ceph-deploy osd create ceph-node4 --data /dev/sdb
# ceph-deploy osd create ceph-node4 --data /dev/sdc
# ceph-deploy osd create ceph-node4 --data /dev/sdd
```

# 删除服务器
- 停止服务器之前要把服务器的 OSD 先停止并从 ceph 集群删除

## 移除故障节点

方法一：
- 1. 先移除节点上所有osd
- 2. ceph osd crush remove `hostname -s`

方法二：
- 1. 先迁移节点上所有osd
- 2. 修改crushmap，删除所有与该节点相关的配置

```
针对这台机器的所有osd进行以下操作：
ceph osd out {osd.num}   标记为out状态，不让该osd继续承载pg
systemctl stop ceph-osd@{osd.num}  停止osd相关进程 状态变为down
ceph osd crush remove osd.{osd.num}   crush map 中删除osd条目
ceph auth del osd.{osd.num}  删除 OSD 认证密钥 
ceph osd rm osd.{osd.num}   删除osd
所有的osd节点下线删除之后：
ceph osd crush remove `hostname -s`  将主机条目从crush map中删除
ceph -s  等待集群变为active+clean状态
```

1.把osd踢出集群
```
# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME            STATUS  REWEIGHT  PRI-AFF
-1         0.23383  root default                                  
-3         0.05846      host ceph-node1                           
 0    hdd  0.01949          osd.0            up   1.00000  1.00000
 1    hdd  0.01949          osd.1            up   1.00000  1.00000
 2    hdd  0.01949          osd.2            up   1.00000  1.00000
-5         0.05846      host ceph-node2                           
 3    hdd  0.01949          osd.3            up   1.00000  1.00000
 4    hdd  0.01949          osd.4            up   1.00000  1.00000
 5    hdd  0.01949          osd.5            up   1.00000  1.00000
-7         0.05846      host ceph-node3                           
 6    hdd  0.01949          osd.6            up   1.00000  1.00000
 7    hdd  0.01949          osd.7            up   1.00000  1.00000
 8    hdd  0.01949          osd.8            up   1.00000  1.00000
-9         0.05846      host node4                                
 9    hdd  0.01949          osd.9            up   1.00000  1.00000
10    hdd  0.01949          osd.10           up   1.00000  1.00000
11    hdd  0.01949          osd.11           up   1.00000  1.00000

#先标记为out，标记后再次查看状态，可以发现权重置为0了，但状态还是up
# ceph osd out osd.9
marked out osd.9. 

# ceph osd out osd.10
marked out osd.10. 

# ceph osd out osd.11
marked out osd.11.
```

2.等一段时间
一段时间后，权重置为0
```
# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME            STATUS  REWEIGHT  PRI-AFF
-1         0.23383  root default                                  
-3         0.05846      host ceph-node1                           
 0    hdd  0.01949          osd.0            up   1.00000  1.00000
 1    hdd  0.01949          osd.1            up   1.00000  1.00000
 2    hdd  0.01949          osd.2            up   1.00000  1.00000
-5         0.05846      host ceph-node2                           
 3    hdd  0.01949          osd.3            up   1.00000  1.00000
 4    hdd  0.01949          osd.4            up   1.00000  1.00000
 5    hdd  0.01949          osd.5            up   1.00000  1.00000
-7         0.05846      host ceph-node3                           
 6    hdd  0.01949          osd.6            up   1.00000  1.00000
 7    hdd  0.01949          osd.7            up   1.00000  1.00000
 8    hdd  0.01949          osd.8            up   1.00000  1.00000
-9         0.05846      host node4                                
 9    hdd  0.01949          osd.9            up         0  1.00000
10    hdd  0.01949          osd.10           up         0  1.00000
11    hdd  0.01949          osd.11           up         0  1.00000
```

3.停止osd.x进程
- 要先去对应的节点上停止ceph-osd服务，否则rm不了
```
# systemctl stop ceph-osd@9.service 
# systemctl stop ceph-osd@10.service 
# systemctl stop ceph-osd@11.service 
```

4.删除osd
- 停止了对应的osd服务，状态会从up变为down，再进行rm，状态会再进行变化成DNE
```
# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME            STATUS  REWEIGHT  PRI-AFF
-1         0.23383  root default                                  
-3         0.05846      host ceph-node1                           
 0    hdd  0.01949          osd.0            up   1.00000  1.00000
 1    hdd  0.01949          osd.1            up   1.00000  1.00000
 2    hdd  0.01949          osd.2            up   1.00000  1.00000
-5         0.05846      host ceph-node2                           
 3    hdd  0.01949          osd.3            up   1.00000  1.00000
 4    hdd  0.01949          osd.4            up   1.00000  1.00000
 5    hdd  0.01949          osd.5            up   1.00000  1.00000
-7         0.05846      host ceph-node3                           
 6    hdd  0.01949          osd.6            up   1.00000  1.00000
 7    hdd  0.01949          osd.7            up   1.00000  1.00000
 8    hdd  0.01949          osd.8            up   1.00000  1.00000
-9         0.05846      host node4                                
 9    hdd  0.01949          osd.9          down         0  1.00000
10    hdd  0.01949          osd.10         down         0  1.00000
11    hdd  0.01949          osd.11         down         0  1.000

# ceph osd rm osd.9
removed osd.9
# ceph osd rm osd.10
removed osd.10
r# ceph osd rm osd.11
removed osd.11

# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME            STATUS  REWEIGHT  PRI-AFF
-1         0.23383  root default                                  
-3         0.05846      host ceph-node1                           
 0    hdd  0.01949          osd.0            up   1.00000  1.00000
 1    hdd  0.01949          osd.1            up   1.00000  1.00000
 2    hdd  0.01949          osd.2            up   1.00000  1.00000
-5         0.05846      host ceph-node2                           
 3    hdd  0.01949          osd.3            up   1.00000  1.00000
 4    hdd  0.01949          osd.4            up   1.00000  1.00000
 5    hdd  0.01949          osd.5            up   1.00000  1.00000
-7         0.05846      host ceph-node3                           
 6    hdd  0.01949          osd.6            up   1.00000  1.00000
 7    hdd  0.01949          osd.7            up   1.00000  1.00000
 8    hdd  0.01949          osd.8            up   1.00000  1.00000
-9         0.05846      host node4                                
 9    hdd  0.01949          osd.9           DNE         0         
10    hdd  0.01949          osd.10          DNE         0         
11    hdd  0.01949          osd.11          DNE         0

#在crush算法中和auth验证中删除
# ceph osd crush remove osd.9
removed item id 9 name 'osd.9' from crush map

# ceph osd crush remove osd.10
removed item id 10 name 'osd.10' from crush map

# ceph osd crush remove osd.11
removed item id 11 name 'osd.11' from crush map

# ceph auth del osd.9

# ceph auth del osd.10

# ceph auth del osd.11

# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME            STATUS  REWEIGHT  PRI-AFF
-1         0.17537  root default                                  
-3         0.05846      host ceph-node1                           
 0    hdd  0.01949          osd.0            up   1.00000  1.00000
 1    hdd  0.01949          osd.1            up   1.00000  1.00000
 2    hdd  0.01949          osd.2            up   1.00000  1.00000
-5         0.05846      host ceph-node2                           
 3    hdd  0.01949          osd.3            up   1.00000  1.00000
 4    hdd  0.01949          osd.4            up   1.00000  1.00000
 5    hdd  0.01949          osd.5            up   1.00000  1.00000
-7         0.05846      host ceph-node3                           
 6    hdd  0.01949          osd.6            up   1.00000  1.00000
 7    hdd  0.01949          osd.7            up   1.00000  1.00000
 8    hdd  0.01949          osd.8            up   1.00000  1.00000
-9               0      host node4
```


```
# ceph osd crush remove node4
ID  CLASS  WEIGHT   TYPE NAME            STATUS  REWEIGHT  PRI-AFF
-1         0.17537  root default                                  
-3         0.05846      host ceph-node1                           
 0    hdd  0.01949          osd.0            up   1.00000  1.00000
 1    hdd  0.01949          osd.1            up   1.00000  1.00000
 2    hdd  0.01949          osd.2            up   1.00000  1.00000
-5         0.05846      host ceph-node2                           
 3    hdd  0.01949          osd.3            up   1.00000  1.00000
 4    hdd  0.01949          osd.4            up   1.00000  1.00000
 5    hdd  0.01949          osd.5            up   1.00000  1.00000
-7         0.05846      host ceph-node3                           
 6    hdd  0.01949          osd.6            up   1.00000  1.00000
 7    hdd  0.01949          osd.7            up   1.00000  1.00000
 8    hdd  0.01949          osd.8            up   1.00000  1.00000
```
