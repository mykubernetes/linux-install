## Ceph在扩容或缩容期间会有数据rebalance。如何控制在rebalance时，尽量降低对client IO的影响？

本质上，用户数据写入ceph时，会被切分成大小相等的object，这些object由PG承载，分布到不同的OSD上（每个OSD一般会对应一块硬盘）。数据的迁移会以PG为单位进行，所以当PG发生变化时，就会有数据rebalance。

后端的数据均衡IO会对client的IO造成影响从而影响到集群的业务IO，所以我们需要对数据均衡IO进行控制，主要是业务优先和恢复优先。


## 那么在什么时候PG会变化呢？

从用户使用角度讲一般有如下几种场景：
- 1、osd暂时下线，然后又上线
- 2、osd硬件故障下线,更换硬盘重新上线

无论哪种情况，osd上线后通常会发现，自己承载的pg有数据落后了，需要进入恢复模式，从其它osd上获取新的数据达到同步。这个过程就是recovery。

recovery分为两种：
- log-based recovery: 是说osd故障时间不长，需要恢复的数据可以通过pg log回放找回来。
- backfill recovery: 是说无法通过pg log回放找全数据，只能通过全量回填(backfill)拷贝。

```
osd_min_pg_log_entries 正常情况下PGLog的记录的条数，
osd_max_pg_log_entries 异常情况下pglog记录的条数，达到该限制会进行trim操作
```



# recovery相关参数
```
osd_max_backfills:默认值10. 一个osd上承载了多个pg。可能很多pg都需要做第二种recovery,即backfill。 设定这个参数来指明在一个osd上最多能有多少个pg同时做backfill。
osd_recovery_max_active：默认值15. 一个osd上可以承载多个pg, 可能好几个pg都需要recovery,这个值限定该osd最多同时有多少pg做recovery。
osd_recovery_max_single_start：默认值5. 这个值限定了每个pg可以启动recovery操作的最大数。
osd_recovery_max_chunk: 默认值8388608. 设置恢复数据块的大小，以防网络阻塞
osd_recovery_op_priority: 默认值10. osd修复操作的优先级, 可小于该值
osd_recovery_sleep: 默认值0. revocery的间隔
```

# 默认配置参数：
```
"osd_max_backfills": "1",
"osd_recovery_sleep": "0",
"osd_recovery_max_active": "3",
"osd_recovery_max_single_start": "1",
```

## 推荐配置参数：

级别:
- 5%是业务优先，对业务影响最小；
- 100%恢复优先，对业务影响最大；

其他介于二者之间；

| 级别 | osd_max_backfills | osd_recovery_max_active | osd_recovery_max_single_start | osd_recovery_sleep | osd_min_pg_log_entries | osd_max_pg_log_entries |
|-----|------|-------|-------|------|-------|---------|
| 5% | 1 | 1 | 1 | 1 | 1 | 2 |
| 25% | 50 | 5 | 5 | 0.25 | 1 | 2 |
| 50% | 50 | 5 | 5 | 0.15 | 1 | 2 |
| 75% | 50 | 5 | 5 | 0 | 1 | 2 |
| 100% | 50 | 5 | 5 | 0 | 1500 | 10000 |



# 调整数据同步参数

1、当调整PG/PGP的值时，会引发ceph集群的backfill操作，数据会以最快的速度进行平衡，因此可能导致集群不稳定。因此首先设置backfill ratio到一个比较小的值。通过下面的命令设置

1.1) 业务优先
```
# ceph tell osd.* injectargs '--osd-max-backfills 1'
# ceph tell osd.* injectargs '--osd-recovery-max-active 1'
# ceph tell osd.* injectargs '--osd-recovery-max-single-start 1'
```
- osd_max_backfills: 一个osd上最多能有多少个pg同时做backfill。其中osd出去的最大backfill数量为osd_max_backfills ，osd进来的最大backfill数量也是osd_max_backfills 
- osd-recovery-max-active: 每个OSD上同时进行的所有PG的恢复操作（active recovery）的最大数量
- osd_recovery_max_single_start: OSD在某个时刻会为一个PG启动恢复操作数

1.2)恢复优先
```
# ceph tell osd.* injectargs '--osd-max-backfills 5'
# ceph tell osd.* injectargs '--osd-recovery-max-active 5'
# ceph tell osd.* injectargs '--osd-recovery-max-single-start 5'
```

2、此外，还包括如下这些参数
```
# ceph tell osd.* injectargs '--osd-backfill-scan-min 2' 
# ceph tell osd.* injectargs '--osd-backfill-scan-max 4' 
# ceph tell osd.* injectargs '--osd-recovery-threads 1' 
# ceph tell osd.* injectargs '--osd-recovery-op-priority 1' 
```

3、注： 在设置之前我们最好先通过如下方式获取到对应参数的原始值，以便在恢复之后能够调整回来
```
# ceph daemon osd.0 config show | grep backfill
    "osd_max_backfills": "2",
    "osd_backfill_full_ratio": "0.85",
    "osd_backfill_retry_interval": "10",
    "osd_backfill_scan_min": "2",
    "osd_backfill_scan_max": "4",
    "osd_kill_backfill_at": "0",
    "osd_debug_skip_full_check_in_backfill_reservation": "false",
    "osd_debug_reject_backfill_probability": "0",

# ceph daemon osd.0 config show | grep recovery
    "osd_min_recovery_priority": "0",
    "osd_allow_recovery_below_min_size": "true",
    "osd_recovery_threads": "1",
    "osd_recovery_thread_timeout": "30",
    "osd_recovery_thread_suicide_timeout": "300",
    "osd_recovery_sleep": "0",
    "osd_recovery_delay_start": "0",
    "osd_recovery_max_active": "2",
    "osd_recovery_max_single_start": "5",
    "osd_recovery_max_chunk": "33554432",
    "osd_recovery_max_omap_entries_per_chunk": "64000",
    "osd_recovery_forget_lost_objects": "false",
    "osd_scrub_during_recovery": "true",
    "osd_recovery_op_priority": "3",
    "osd_recovery_op_warn_multiple": "16",
```

4、在调整完成之后执行如下命令进行参数恢复：
```
# ceph tell osd.* injectargs '--osd-max-backfills 2'
# ceph tell osd.* injectargs '--osd-recovery-max-active 2'
# ceph tell osd.* injectargs '--osd-recovery-max-single-start 5'

# ceph tell osd.* injectargs '--osd-backfill-scan-min 2' 
# ceph tell osd.* injectargs '--osd-backfill-scan-max 4' 
# ceph tell osd.* injectargs '--osd-recovery-threads 1' 
# ceph tell osd.* injectargs '--osd-recovery-op-priority 3' 
```

5、在业务繁忙时，完全关闭数据重建及迁移
```
ceph osd set norebalance
ceph osd set norecover
ceph osd set nobackfill
```
 
 
6、在业务空闲时，打开数据重建及迁移
```
ceph osd unset norebalance
ceph osd unset norecover
ceph osd unset nobackfill
```


