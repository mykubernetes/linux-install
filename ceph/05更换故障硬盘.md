一、移除故障磁盘  
---

1、移除一个磁盘，查看osd状态
```
# ceph osd tree
ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF
-1 0.17537 root default
-3 0.05846 host node01
 0 hdd 0.01949 osd.0 up 1.00000 1.00000
 3 hdd 0.01949 osd.3 down 1.00000 1.00000
 6 hdd 0.01949 osd.6 up 1.00000 1.00000
...
```

2、查看集群状态
```
# ceph status
  cluster:
    id: 15484ab5-ddb2-4a54-b9b3-ed8674bc5d05
    health: HEALTH_WARN
            1 osds down
            Degraded data redundancy: 21/186 objects degraded (11.290%),
17 pgs degraded
            application not enabled on 1 pool(s)
  services:
    mon: 3 daemons, quorum node01,node02,nonde03
    mgr: node01(active), standbys: node02,node03
    mds: cephfs-1/1/1 up {0=c720178=up:active}, 2 up:standby
    osd: 9 osds: 8 up, 9 in
...
```
- 可能看到提示还有9个in，目前，它不会触发此驱动器的数据恢复，一般需要300s才将磁盘标记为Out ，然后触发数据恢复。此超时的原因是为了避免由于短期中断而导致的不必要的数据移动。你如果不想等待 300s，可以手工标记为 Out  

3、在crush中设置OSD weight为0，等待迁移完成。
```
# ceph osd crush reweight osd.3 0
```

4、可以手工标记为out
```
# ceph osd out osd.3
```

5、停止OSD进程（如果进程还在运行）：
```
# systemctl stop ceph-osd@3
```

6、一旦OSD标记为OUT，Ceph集群将立即启动对故障磁盘上托管的PG的恢复操作。查看集群状态等待数据恢复完成。
```
# ceph status
  cluster:
    id: 15484ab5-ddb2-4a54-b9b3-ed8674bc5d05
    health: HEALTH_WARN
            Reduced data availability: 4 pgs inactive
            Degraded data redundancy: 21/186 objects degraded (11.290%),
8 pgs degraded, 28 pgs undersized
            application not enabled on 1 pool(s)
  services:
    mon: 3 daemons, quorum c720176,c720177,c720178
    mgr: c720176(active), standbys: c720178, c720177
    mds: cephfs-1/1/1 up {0=c720178=up:active}, 2 up:standby
    osd: 9 osds: 8 up, 8 in
...
```  

7、从 Ceph crush map 中删除故障磁盘 OSD
```
# ceph osd crush remove osd.3
```

8、删除OSD的Ceph身份验证密钥
```
# ceph auth del osd.3
```

9、从集群中删除 OSD
```
# ceph osd rm osd.3
```

10、由于其中一个OSD不可用，因此群集运行状况不正常，群集将执行恢复。恢复操作完成后，群集将获得 HEALTH_OK
```
# ceph -s
# ceph osd stat
```


二、添加新磁盘
---

1、查看磁盘列表
```
# ceph-deploy disk list node01
......
[ceph_deploy.osd][DEBUG ] Listing disks on node01...
[node01][DEBUG ] find the location of an executable
[node01][INFO  ] Running command: /usr/sbin/ceph-disk list
[node01][DEBUG ] /dev/dm-0 other, xfs, mounted on /
[node01][DEBUG ] /dev/dm-1 swap, swap
[node01][DEBUG ] /dev/sda :
[node01][DEBUG ] /dev/sda2 other, LVM2_member
[node01][DEBUG ] /dev/sda1 other, xfs, mounted on /boot
[node01][DEBUG ] /dev/sdb :
[node01][DEBUG ] /dev/sdb2 other
[node01][DEBUG ] /dev/sdb1 ceph data, active, cluster ceph, osd.0
[node01][DEBUG ] /dev/sr0 other, unknown
......
```

2、验证磁盘是否处于干净状态
```
# ceph-deploy disk zap node01 /dev/sde    # 清除分区
# lsblk
```

3、添加磁盘到集群中
```
# ceph-deploy osd create node01 --data /dev/sde
# lsblk
```

4、一旦ceph-disk prepare命令完成后，集群将执行回填操作并将开始将PG从辅助OSD移动到新的OSD。恢复操作可能需要一段时间，但在此之后，Ceph集群将HEALTH_OK再次出现3、3
```
# ceph -s
# ceph osd stat
```
