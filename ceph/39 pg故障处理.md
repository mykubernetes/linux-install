# ceph pg 故障处理和日常操作

## 一、pg 异常

### 1，incomplete 状态
```
# ceph health detail | grep 1.287
pg 1.287 is incomplete, acting [13,7,33]

# ceph pg map 1.287 
osdmap e22380 pg 1.287 (1.287) -> up [13,7,33] acting [13,7,33]
```

更改主 OSD 状态 。强制将 pg 状态置为正常（在主 osd 上执行，需要先停止该 osd 进程）：
```
ceph osd set norecover
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-13 --journal-path /dev/disk/by-partuuid/896b1860-11c5-48c0-9aa7-cc98daef3fc5 --pgid 1.287 --op mark-complete
ceph osd unset norecover
```

下面是 /var/log/ceph/ceph-osd.13.log 的部分日志
```
2019-07-12 08:34:11.374480 osd.13 [ERR] 1.287 repair stat mismatch, got 2249/4 objects, 3/1 clones, 2249/4 dirty, 0/0 omap, 0/0 hit_set_archive, 0/0 whiteouts, 9371858944/12718080 bytes,0/0 hit_set_archive bytes.
2019-07-12 08:34:11.374821 osd.13 [ERR] 1.287 repair 1 errors, 1 fixed
```

### 2，backfill_toofull 卡住状态
```
[root@ceph1 ~]# ceph pg map 1.582
osdmap e24286 pg 1.582 (1.582) -> up [4,26,35] acting [4,26,48]   
// upset 是要平衡到的 osd 集合，actingset 是当前使用的 osd 集合

/dev/sda1       1.9T  1.6T  271G  86% /var/lib/ceph/osd/ceph-35

ceph osd df
35 1.75000  1.00000  1862G  1591G   270G 85.48 1.24 

osd.35 is near full at 85% 
```

osd.35 要满了，调一下。
```
[root@ceph1 ~]# ceph -n osd.35 --show-config | grep full_ratio
mon_osd_full_ratio = 0.95
mon_osd_nearfull_ratio = 0.85
osd_backfill_full_ratio = 0.85
osd_pool_default_cache_target_full_ratio = 0.8
osd_failsafe_full_ratio = 0.97
osd_failsafe_nearfull_ratio = 0.9

[root@ceph1 ~]# ceph tell osd.35 injectargs '--osd_backfill_full_ratio=0.87'
osd_backfill_full_ratio = '0.87' 
[root@ceph1 ~]# ceph -n osd.35 --show-config | grep osd_backfill_full_ratio
osd_backfill_full_ratio = 0.85
```

看起来并没有生效，但是为什么数据开始平衡起来了？没有 1 active+remapped+backfill_toofull 了。
下面这个看，其实生效了。
```
[root@ceph3 ~]# ceph --admin-daemon /var/run/ceph/ceph-osd.35.asok config show | grep full_ratio // 或
[root@ceph3 ~]# ceph daemon osd.35 config show | grep full_ratio // 这两条都需要在 osd 所在机器上运行。
    "mon_osd_full_ratio": "0.95",
    "mon_osd_nearfull_ratio": "0.85",
    "osd_backfill_full_ratio": "0.87",
    "osd_pool_default_cache_target_full_ratio": "0.8",
    "osd_failsafe_full_ratio": "0.97",
    "osd_failsafe_nearfull_ratio": "0.9",
```

### 3，active+undersized+degraded 卡住状态
```
# ceph pg dump_stuck 
pg_stat    state    up    up_primary    acting    acting_primary
1.632    active+undersized+degraded    [57,16]    57    [57,16]    57
1.42a    active+undersized+degraded    [2,14]    2    [2,14]    2
4.56active+undersized+degraded    [7,17]    7    [7,17]    7
```

原来的三副本变成了现在的两副本，并卡住了没做 remapped，原因是前面有三个 osd down 且 out 了但还在 crush 里面。

把 osd.50 删除，当执行到 crush remove osd.50 后 ，upset 变成 3 副本了。 注意如果 osd.50 的 weight 不是 0，运行这条就会把 osd.50 的数据数据平衡出去，为减少于业务的影响应该先慢慢把 reweight 调整到 0 后再 crush remove。
```
# ceph pg dump_stuck 
pg_stat    state    up    up_primary    acting    acting_primary
1.632    active+undersized+degraded+remapped    [57,16,34]    57    [57,16]    57
1.42a    active+undersized+degraded+remapped    [2,14,48]    2    [2,14]    2
4.56    active+undersized+degraded+remapped    [7,17,34]    7    [7,17]    7
```

### 4，down+peering 卡住状态
```
[root@ceph1 ~]# ceph health detail | grep down+peering
pg 1.287 is stuck inactive for 1620363.085798, current state down+peering, last acting [13,7,33]
pg 1.287 is stuck unclean for 1620363.087043, current state down+peering, last acting [13,7,33]
pg 1.287 is down+peering, acting [13,7,33]
```

查看 13,7,33 三个 osd 都是 up 的。
```
[root@ceph1 ~]# ceph pg 1.287 query
...
recovery_state 节里有
 "blocked": "peering is blocked due to down osds",
            "down_osds_we_would_probe": [
                29
            ],
            "peering_blocked_by": [
                {
                    "osd": 29,
                    "current_lost_at": 0,
                    "comment": "starting or marking this osd lost may let us proceed" 
                }
            ]
...
```

查到原因是 osd.29 down 了。

因为之前已经把 osd.29 的 weight 调到 0 把数据迁移走了，下面直接干。
```
[root@ceph1 ~]# ceph osd lost 29 --yes-i-really-mean-it
```
如果还不行就把 osd.29 删除。


### 5，unfound 状态
```
ceph pg 1.6da list_missing
ceph pg map 1.6da
ceph pg 1.6da query // 里面查到的丢失 oid 可以用 rbd info 的方式查询出属于哪个 rbd 卷。
ceph pg 1.6da mark_unfound_lost revert
```

```
ceph pg 1.2af mark_unfound_lost delete  // pg query 里查到的 backfill_targets 的 osd 会 down 掉。安静的等它平衡完吧。
pg has 6 objects unfound and apparently lost, marking

---
1. For a new object without a previous version:
# ceph pg {pg.num} mark_unfound_lost delete

2. For an object which is likely to have a previous version:
# ceph pg {pg.num} mark_unfound_lost revert
```

### 6，inconsistent 状态
```
ceph --cluster xxx pg repair 2.82
```

## 二、pg 常用操作

### 1，查询 block_name_prefix 属于哪个 rbd

特别是在处理 unfound 时，用ceph pg {pgid} list_missing 查询出相应的 oid 后，通过此方法去查找影响到哪个 rbd 。
```
// 遍历 images 沲输出到文件，然后通过文件来查找
rbd --pool image ls | while read image ; do rbd --pool image info $image; done | tee -a rbd.pool.image.info
```

### 2，获取并过滤 pg 详细信息
```
# ceph pg dump dump 2> /dev/null | egrep "pg_stat|clean" | awk '{printf "%-8s %-8s %-16s %-16s %-16s %-64s\n", $1,$2,$7,$15,$17,$10}'
pg_stat  objects  bytes            up_primary       acting_primary   state  
1.615    2166     9033587200       [48,22,55]       [48,22,55]       active+clean 
2.616    159      660242432        [36,44,3]        [36,44,3]        active+clean
1.612    2292     9542796288       [4,17,51]        [4,17,51]        active+clean
2.611    204      851419136        [36,39,33]       [36,39,33]       active+clean 
```

### 3，导出 pg

导出前要把所在的 osd 停止
```
ceph-objectstore-tool --data-path /var/lib/ceph/osd/ceph-29/ --journal-path /dev/disk/by-partuuid/9433b4b3-01d0-4b3b-90a8-6e60a945fa2e --pgid 1.6da --op export --file osd.29_pg1.6da.export
```
